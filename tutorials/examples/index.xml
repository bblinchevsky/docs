<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>– Examples</title><link>https://trustgrid.github.io/docs/tutorials/examples/</link><description>Recent content in Examples on</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://trustgrid.github.io/docs/tutorials/examples/index.xml" rel="self" type="application/rss+xml"/><item><title>Tutorials: Edge Deployment Scenarios</title><link>https://trustgrid.github.io/docs/tutorials/examples/edge-deployment-scenarios/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://trustgrid.github.io/docs/tutorials/examples/edge-deployment-scenarios/</guid><description>
&lt;p>When deciding on an Edge deployment architecture, there are several key variables to consider:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Behind the Firewall or Beside the Firewall&lt;/p>
&lt;/li>
&lt;li>
&lt;p>HA or Single &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">Node&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multi-interface or Single-interface&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Layer 3 or Layer 4 Edge &lt;a href="https://trustgrid.github.io/docs/getting-started/networking/">networking&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Trustgrid supports unique configurations per site to minimize the changes required at the Edge network. The easiest scenario to deploy is a Layer 4, behind the firewall, single &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node&lt;/a> deployment. This is the standard recommendation. When deployed behind a firewall, the firewall must allow egress traffic to Trustgrid&amp;rsquo;s Cloud environment (see Network Requirements) and the &lt;a href="https://trustgrid.github.io/docs/getting-started/networking/">network(s)&lt;/a> where the gateway nodes are installed.&lt;/p>
&lt;h3 id="beside-a-firewall">Beside a Firewall&lt;/h3>
&lt;p>In some scenarios, you may want to deploy your Trustgrid &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> beside the firewall. In this configuration, the management interface of each &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node&lt;/a> will be connected directly to the internet and the data interface will get connected to a LAN such as the inside/LAN network.&lt;/p>
&lt;h5 id="edge-network-topology">Edge Network Topology:&lt;/h5>
&lt;p>&lt;img src="edge-topology.png" alt="img">&lt;/p>
&lt;h3 id="considerations-in-this-configuration">Considerations in this Configuration:&lt;/h3>
&lt;ul>
&lt;li>A unique, static, public IP address is required for each &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node&lt;/a>.&lt;/li>
&lt;li>Typical deployments beside a firewall involve a switch connected to the ISP’s ethernet handoff, the firewall, and the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">nodes&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://trustgrid.github.io/docs/docs/clusters/">Clustered nodes&lt;/a> in this configuration may require using the data interface for &lt;a href="https://trustgrid.github.io/docs/docs/clusters/">cluster&lt;/a> communication.&lt;/li>
&lt;li>It is possible to configure L4 proxy connectors that are bound to the management interface. Care should be taken to ensure that L4 proxy connectors aren’t inadvertently configured to allow access to a service from the public internet. If using L4 proxy connectors on the management interface, the source CIDR should be used to limit access to the service as desired.&lt;/li>
&lt;/ul>
&lt;h4 id="requirements-to-configure">Requirements to Configure:&lt;/h4>
&lt;ul>
&lt;li>Standard
&lt;ul>
&lt;li>1 Public IP Address&lt;/li>
&lt;li>1 Private IP Address&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>HA
&lt;ul>
&lt;li>2 Public IP Addresses&lt;/li>
&lt;li>2 Private IP Addresses&lt;/li>
&lt;li>1 Private IP Address for Cluster IP&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="behind-a-firewall">Behind a Firewall&lt;/h3>
&lt;p>The most typical configuration for edge nodes is to place the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">nodes&lt;/a> behind a firewall with the appropriate access controls in place to allow communication on the management interface. In this configuration, the management configuration can be placed in a DMZ network and the data interface would get connected to a LAN such as the inside/LAN network.&lt;/p>
&lt;h5 id="edge-network-topology-1">Edge Network Topology:&lt;/h5>
&lt;p>&lt;img src="edge-topology2.png" alt="img">&lt;/p>
&lt;h4 id="considerations-in-this-configuration-1">Considerations in this Configuration:&lt;/h4>
&lt;ul>
&lt;li>Multiple &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">nodes&lt;/a> can NAT to a single public IP address.&lt;/li>
&lt;li>Firewalls in front of gateway nodes must be configured to allow access to the IP:Port of the gateway &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">nodes&lt;/a> from the public IP address(es) that the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> NAT to.&lt;/li>
&lt;li>The firewall must be configured to allow outbound traffic from the management interface of the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> based on Trustgrid&amp;rsquo;s network requirements.&lt;/li>
&lt;/ul>
&lt;h4 id="requirements-to-configure-1">Requirements to Configure:&lt;/h4>
&lt;ul>
&lt;li>Standard
&lt;ul>
&lt;li>1 Public IP Address&lt;/li>
&lt;li>1 Private IP Address&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>HA
&lt;ul>
&lt;li>2 Public IP Addresses&lt;/li>
&lt;li>2 Private IP Addresses&lt;/li>
&lt;li>1 Private IP Address for Cluster IP&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="firewalled-nodes">Firewalled Nodes&lt;/h3>
&lt;p>Some organizations may choose to put the management interface behind a firewall and also put a firewall between the data interface and the local &lt;a href="https://trustgrid.github.io/docs/getting-started/networking/">network&lt;/a>.&lt;/p>
&lt;h5 id="edge-network-topology-2">Edge Network Topology:&lt;/h5>
&lt;p>&lt;img src="edge-topology3.png" alt="img">&lt;/p>
&lt;h4 id="considerations-in-this-configuration-2">Considerations in this Configuration:&lt;/h4>
&lt;ul>
&lt;li>Multiple &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">nodes&lt;/a> can NAT to a single public IP address.&lt;/li>
&lt;li>Firewalls in front of gateway &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">nodes&lt;/a> must be configured to allow access to the IP:Port of the gateway nodes from the public IP address(es) that the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> NAT to.&lt;/li>
&lt;li>The firewall between the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> and the internet must be configured to allow outbound traffic from the management interface of the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> based on Trustgrid&amp;rsquo;s network requirements.&lt;/li>
&lt;li>The firewall between the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> and the LAN must allow connectivity from all IP addresses associated with the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> data interface to resources on the necessary services on the LAN. This includes any IP addresses used in outside NAT configurations as well as the cluster IP.&lt;/li>
&lt;/ul>
&lt;h4 id="requirements-to-configure-2">Requirements to Configure:&lt;/h4>
&lt;ul>
&lt;li>Standard
&lt;ul>
&lt;li>1 Public IP Address&lt;/li>
&lt;li>1 Private IP Address&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>HA
&lt;ul>
&lt;li>2 Public IP Addresses&lt;/li>
&lt;li>2 Private IP Addresses&lt;/li>
&lt;li>1 Private IP Address for Cluster IP&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="on-a-stick---single-node-interface">On-a-stick - Single Node Interface&lt;/h3>
&lt;p>Our best practice recommendation is to separate the management and data traffic when possible, but in some cases the use of both network interfaces on edge nodes isn’t desirable. The most common scenario for this configuration is when the edge network is a small, basic, network where utilizing both interfaces on the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node(s)&lt;/a> would mean connecting both interfaces to the same network. In this scenario, an on-a-stick configuration can be used.&lt;/p>
&lt;h5 id="edge-network-topology-3">Edge Network Topology:&lt;/h5>
&lt;p>&lt;img src="edge-topology4.png" alt="img">&lt;/p>
&lt;h4 id="considerations-in-this-configuration-3">Considerations in this Configuration:&lt;/h4>
&lt;ul>
&lt;li>Only the management interface of the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node&lt;/a> is used, requiring a minimum of three IP addresses to support both management/data traffic.&lt;/li>
&lt;li>The management interface must be connected to a network that allows outbound access to the internet based on our network documentation, as well as access to a LAN/inside network.&lt;/li>
&lt;li>Clustered &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">nodes&lt;/a> should be configured with the &lt;a href="https://trustgrid.github.io/docs/docs/clusters/">cluster&lt;/a> communication IP set to the IP of the management interface.&lt;/li>
&lt;/ul>
&lt;h4 id="requirements-to-configure-3">Requirements to Configure:&lt;/h4>
&lt;ul>
&lt;li>Standard
&lt;ul>
&lt;li>1 Private IP Address&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>HA
&lt;ul>
&lt;li>2 Private IP Addresses&lt;/li>
&lt;li>1 Private IP Address for Cluster IP&lt;/li>
&lt;li>2 DNS Servers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hosted-fi-core---access-to-remote-network-via-edge">Hosted FI Core - Access to Remote Network via Edge&lt;/h3>
&lt;p>A common deployment scenario involves the need to access a network that is routable from the edge location, but outside of the &lt;a href="https://trustgrid.github.io/docs/getting-started/networking/">network&lt;/a> that the Trustgrid edge node is configured on. An example of this is a financial institution with a hosted core that may be available from the financial institution over a connection such as a VPN or MPLS.&lt;/p>
&lt;h5 id="edge-network-topology-4">Edge Network Topology:&lt;/h5>
&lt;p>&lt;img src="edge-topology5.png" alt="img">&lt;/p>
&lt;h4 id="considerations-in-this-configuration-4">Considerations in this Configuration:&lt;/h4>
&lt;ul>
&lt;li>Inside NAT(s) for the remote address should be added on the data interface to map the hosted core IP address(es) to virtual IPs on the &lt;a href="https://trustgrid.github.io/docs/docs/domain/virtual-networks/">Trustgrid virtual network&lt;/a>.&lt;/li>
&lt;li>In this scenario setting a &lt;a href="https://trustgrid.github.io/docs/docs/domain/virtual-networks/routes/">route&lt;/a> on the data interface from the Trustgrid portal may be necessary.&lt;/li>
&lt;li>In most cases the hosted core provider will need to allow access to the hosted core from any IP addresses associated with a &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node&amp;rsquo;s&lt;/a> data interface. This includes any IP addresses used in outside NAT configurations as well as the cluster IP.&lt;/li>
&lt;/ul>
&lt;h3 id="outside-nat---avoiding-routing-at-the-edge">Outside NAT - Avoiding Routing at the Edge&lt;/h3>
&lt;p>In some scenarios, it may not be desirable to have to add &lt;a href="https://trustgrid.github.io/docs/docs/domain/virtual-networks/routes/">routes&lt;/a> on the edge network in order to &lt;a href="https://trustgrid.github.io/docs/docs/domain/virtual-networks/routes/">route&lt;/a> traffic through the Trustgrid network. We recommend using L4 proxy in these scenarios when possible, however outside NAT can be utilized to allow traffic to and from the edge network without adding &lt;a href="https://trustgrid.github.io/docs/docs/domain/virtual-networks/routes/">routes&lt;/a> on the edge network.&lt;/p>
&lt;h5 id="edge-network-topology-5">Edge Network Topology:&lt;/h5>
&lt;p>&lt;img src="edge-topology6.png" alt="img">&lt;/p>
&lt;h4 id="considerations-in-this-configuration-5">Considerations in this Configuration:&lt;/h4>
&lt;ul>
&lt;li>In this configuration you can configure PAT, which allows you to source all traffic from the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node&lt;/a> on a single IP address that exists on the local network.&lt;/li>
&lt;li>The IP address that you select must be a free IP address on the network that is also excluded from any DHCP scope on the subnet that it&amp;rsquo;s in.&lt;/li>
&lt;li>In some cases it may be desirable to use multiple outside NATs in order to identify traffic from a gateway-side host using a unique source IP at the edge. Using more than a handful of outside NATs is not recommended as it may present a problem with connectivity during a HA failover situation.&lt;/li>
&lt;/ul>
&lt;h3 id="l3-to-l4-proxying---function-with-1-ip">L3 to L4 Proxying - Function with 1 IP&lt;/h3>
&lt;p>In deployments where only a few services need to be accessed via an edge node, it may make sense to deploy the edge node in a mode in which the edge node intercepts layer 3 TCP traffic and proxies the traffic to a TCP service defined in the L4 Proxy configuration. The advantage of configuring the edge node in this fashion is that the traffic effectively does a source pat out of the &lt;a href="https://trustgrid.github.io/docs/docs/nodes/">node&lt;/a> interface IP address, thus simplifying the network configuration at the edge node location. The edge node appears to the server as a single client machine accessing the server, rather than a range of different IP clients for which the network in that location must be configured to forward traffic to. One of the biggest differences between this and the previous outside nat configuration is that all traffic is coming from the single interface IP address, rather than a separate IP address.&lt;/p>
&lt;h4 id="considerations-in-this-configuration-6">Considerations in this Configuration:&lt;/h4>
&lt;ul>
&lt;li>In this configuration all traffic for a configured service will come from a single IP address. If you need to differentiate different clients by IP address, this configuration will not suffice.&lt;/li>
&lt;li>Only TCP services can be used in this configuration. UDP and ICMP will not be usable with this configuration.&lt;/li>
&lt;/ul></description></item><item><title>Tutorials: Layer 3 - HA Edge to AWS</title><link>https://trustgrid.github.io/docs/tutorials/examples/layer3-ha-edge-to-aws/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://trustgrid.github.io/docs/tutorials/examples/layer3-ha-edge-to-aws/</guid><description>
&lt;p>In this scenario a clustered pair of active/passive edge nodes at each edge location connects to a pair of active/passive clustered gateway nodes in an AWS VPC and layer 3 traffic is routed between the edge location and AWS.&lt;/p>
&lt;p>This configuration allows for the loss of one node at each site without a loss of connectivity between the edge location and AWS. Upon node failover, the active cluster member is switched to the second node and the VPC route table is updated to route traffic destined for the Trustgrid network through the ENI of the second node, allowing traffic flow to continue. Failback to the original active cluster member happens when the original active node recovers from the event that caused the failover, and the VPC route table is updated to route traffic destined fro the Trustgrid network through the ENI of the data NIC on the original active node.&lt;/p>
&lt;p>Inside NATs are used to translate local IP addresses to IP addresses on the Trustgrid network.&lt;/p>
&lt;p>&lt;img src="aws-topology.png" alt="Network Topology">&lt;/p>
&lt;h3 id="network-information">Network Information&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local Network CIDR&lt;/th>
&lt;th>Location&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.0/24&lt;/td>
&lt;td>AWS VPC Subnet&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.0/24&lt;/td>
&lt;td>Edge location 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.0/24&lt;/td>
&lt;td>Edge location 2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.0/24&lt;/td>
&lt;td>Edge location 3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="route-configuration">Route Configuration&lt;/h3>
&lt;p>In this example, the Trustgrid network is &lt;code>10.100.0.0/16&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Trustgrid Network CIDR&lt;/th>
&lt;th>Destination Node&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10.100.100.0/24&lt;/td>
&lt;td>AWSCluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.10.0/24&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.20.0/24&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.30.0/24&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inside-nat-configuration">Inside NAT Configuration&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local CIDR&lt;/th>
&lt;th>Network CIDR&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.15/32&lt;/td>
&lt;td>10.100.100.15/32&lt;/td>
&lt;td>AWSCluster&lt;/td>
&lt;td>Application server behind AWSCluster nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.15/32&lt;/td>
&lt;td>10.100.10.15/32&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;td>MySQL server behind EdgeCluster1 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.15/32&lt;/td>
&lt;td>10.100.20.15/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>MySQL server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.20/32&lt;/td>
&lt;td>10.100.20.20/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>SFTP server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.15/32&lt;/td>
&lt;td>10.100.30.15/32&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;td>MySQL server behind EdgeCluster3 nodes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Tutorials: Layer 3 - HA Edge to DC</title><link>https://trustgrid.github.io/docs/tutorials/examples/layer3-ha-edge-to-dc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://trustgrid.github.io/docs/tutorials/examples/layer3-ha-edge-to-dc/</guid><description>
&lt;p>In this scenario, a clustered pair of active/passive edge nodes at each edge location connects to a pair of active/passive clustered gateway nodes at the data center and layer 3 traffic is routed between the edge location and the data center.&lt;/p>
&lt;p>This configuration allows for the loss of one node at each site without a loss of connectivity between the edge location and the data center. Upon node failover, the active cluster member is switched to the second node and the shared IP address follows, allowing traffic flow to continue using the second node. Failback to the original active node happens when the original active recovers from the event that caused the failover, and the shared IP is moved back to the original active at that time.&lt;/p>
&lt;p>A route is added in the core router at the data center and edge locations for the Trustgrid subnet with the next hop set to the shared IP of the clustered nodes in each location. Inside NATs are used to translate local IP addresses to IP addresses on the Trustgrid network.&lt;/p>
&lt;p>&lt;img src="l3-ha-dc-topology.png" alt="Network Topology">&lt;/p>
&lt;h3 id="network-information">Network Information&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local Network CIDR&lt;/th>
&lt;th>Location&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.0/24&lt;/td>
&lt;td>Data center&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.0/24&lt;/td>
&lt;td>Edge location 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.0/24&lt;/td>
&lt;td>Edge location 2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.0/24&lt;/td>
&lt;td>Edge location 3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="route-configuration">Route Configuration&lt;/h3>
&lt;p>In this example, the Trustgrid network is &lt;code>10.100.0.0/16&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Trustgrid Network CIDR&lt;/th>
&lt;th>Destination Node&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10.100.100.0/24&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.10.0/24&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.20.0/24&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.30.0/24&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inside-nat-configuration">Inside NAT Configuration&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local CIDR&lt;/th>
&lt;th>Network CIDR&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.15/32&lt;/td>
&lt;td>10.100.100.15/32&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;td>Application server behind DCCluster nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.15/32&lt;/td>
&lt;td>10.100.10.15/32&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;td>MySQL server behind EdgeCluster1 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.15/32&lt;/td>
&lt;td>10.100.20.15/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>MySQL server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.20/32&lt;/td>
&lt;td>10.100.20.20/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>SFTP server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.15/32&lt;/td>
&lt;td>10.100.30.15/32&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;td>MySQL server behind EdgeCluster3 nodes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Tutorials: Layer 3 - HA Edge to Hybrid Cloud</title><link>https://trustgrid.github.io/docs/tutorials/examples/layer3-ha-edge-to-hybrid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://trustgrid.github.io/docs/tutorials/examples/layer3-ha-edge-to-hybrid/</guid><description>
&lt;p>In this scenario, a clustered pair of active/passive edge nodes at each edge location connects to a pair of active/passive clustered gateway nodes in both an AWS VPC and a data center with layer 3 routed between the edge locations and the hybrid cloud.&lt;/p>
&lt;p>This configuration allows for the loss of one node at each site without a loss of connectivity between the edge location and the hybrid cloud. Clustered nodes at the edge locations and the data center use a shared IP. The clustered nodes in the AWS VPC maintain the correct ENI as the target for the route to the Trustgrid network in the VPC route table to allow traffic to continue to flow in the event of a failover on any of the nodes. Failback to the original active cluster member happens when the node recovers from the event that caused the failover, and on the data center or edge nodes the shared IP is switched back to the original active node.&lt;/p>
&lt;p>Failback on AWS nodes triggers an update of the route table to set the target for the route to the Trustgrid network back to the ENI of the data NIC on the original active cluster member. Inside NATs are used to translate local IP addresses to IP addresses on the Trustgrid network.&lt;/p>
&lt;p>A route is added in the core router at the data center and edge locations for the Trustgrid subnet with the next hop set to the shared IP of the clustered nodes in each location. Inside NATs are used to translate local IP addresses to IP addresses on the Trustgrid network.&lt;/p>
&lt;p>&lt;img src="hybrid-topology.png" alt="Network Topology">&lt;/p>
&lt;h3 id="network-information">Network Information&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local Network CIDR&lt;/th>
&lt;th>Location&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.0/24&lt;/td>
&lt;td>AWS VPC Subnet&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>192.168.20.0/24&lt;/td>
&lt;td>Data center&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.0/24&lt;/td>
&lt;td>Edge location 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.0/24&lt;/td>
&lt;td>Edge location 2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="route-configuration">Route Configuration&lt;/h3>
&lt;p>In this example, the Trustgrid network is &lt;code>10.100.0.0/16&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Trustgrid Network CIDR&lt;/th>
&lt;th>Destination Node&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10.100.100.0/24&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.200.0/24&lt;/td>
&lt;td>AWSCluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.10.0/24&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.20.0/24&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inside-nat-configuration">Inside NAT Configuration&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local CIDR&lt;/th>
&lt;th>Network CIDR&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.15/32&lt;/td>
&lt;td>10.100.100.15/32&lt;/td>
&lt;td>AWSCluster&lt;/td>
&lt;td>Application server behind AWSCluster nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>192.168.20.15/32&lt;/td>
&lt;td>10.100.200.15/32&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;td>Application server behind DCCluster nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.15/32&lt;/td>
&lt;td>10.100.10.15/32&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;td>MySQL server behind EdgeCluster1 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.15/32&lt;/td>
&lt;td>10.100.20.15/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>MySQL server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.20/32&lt;/td>
&lt;td>10.100.20.20/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>SFTP server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Tutorials: Layer 3 and Layer 4 - HA Edge to DC</title><link>https://trustgrid.github.io/docs/tutorials/examples/layer3-layer4-ha-edge-to-dc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://trustgrid.github.io/docs/tutorials/examples/layer3-layer4-ha-edge-to-dc/</guid><description>
&lt;p>In this scenario, a clustered pair of active/passive edge nodes at each edge location connects to a pair of active/passive clustered gateway nodes at the data center. Layer 3 is utilized to allow an application server at the data center to access data at the edge location and a layer 4 service and connector is used at the edge locations to provide connectivity to an FTP service back at the data center.&lt;/p>
&lt;p>This configuration allows for the loss of one node at each site without a loss of connectivity between the edge location and the data center. Upon node failover, the active cluster member is switched to the second node and the shared IP address follows, allowing traffic to continue to flow using the second node. Failback to the original active node happens when the node recovers from the event that caused the failover and the shared IP is moved back to the original node at that time.&lt;/p>
&lt;p>A route is added in the core router at the data center and edge locations for the Trustgrid subnet with the next hop set to the shared IP of the clustered nodes at each location. Inside NATs are used to translate local IP addresses to IP addresses on the Trustgrid network for layer 3 traffic. The L4 service on the data center node is configured so that L4 traffic to the FTP server will show up with the source IP as the shared IP for the cluster. The listening port of the L4 connector is accessible via the shared IP on the edge nodes, allowing for high-availability of the L4 connector.&lt;/p>
&lt;p>&lt;img src="l4-topology.png" alt="Network Topology">&lt;/p>
&lt;h3 id="layer-4-configuration">Layer 4 Configuration&lt;/h3>
&lt;h4 id="service">Service&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Name&lt;/th>
&lt;th>Destination&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FTP&lt;/td>
&lt;td>DCFTP&lt;/td>
&lt;td>192.168.10.15:21&lt;/td>
&lt;td>FTP access on DC application server&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="connector">Connector&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Listen Port&lt;/th>
&lt;th>Source Block&lt;/th>
&lt;th>Destination Node&lt;/th>
&lt;th>Remote Address&lt;/th>
&lt;th>IP&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FTP&lt;/td>
&lt;td>8021&lt;/td>
&lt;td>&lt;CIDR of edge server>&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;td>svc://DCFTP&lt;/td>
&lt;td>&lt;Cluster shared IP>&lt;/td>
&lt;td>L4 connector for FTP access from each edge node&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="network-information">Network Information&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local Network CIDR&lt;/th>
&lt;th>Location&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.0/24&lt;/td>
&lt;td>Data center&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.0/24&lt;/td>
&lt;td>Edge location 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.0/24&lt;/td>
&lt;td>Edge location 2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.0/24&lt;/td>
&lt;td>Edge location 3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="route-configuration">Route Configuration&lt;/h3>
&lt;p>In this example, the Trustgrid network is &lt;code>10.100.0.0/16&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Trustgrid Network CIDR&lt;/th>
&lt;th>Destination Node&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10.100.100.0/24&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.10.0/24&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.20.0/24&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.30.0/24&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inside-nat-configuration">Inside NAT Configuration&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local CIDR&lt;/th>
&lt;th>Network CIDR&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.15/32&lt;/td>
&lt;td>10.100.100.15/32&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;td>Application server behind DCCluster nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.15/32&lt;/td>
&lt;td>10.100.10.15/32&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;td>MySQL server behind EdgeCluster1 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.15/32&lt;/td>
&lt;td>10.100.20.15/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>MySQL server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.20/32&lt;/td>
&lt;td>10.100.20.20/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>SFTP server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.15/32&lt;/td>
&lt;td>10.100.30.15/32&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;td>MySQL server behind EdgeCluster3 nodes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Tutorials: Layer 3 and Layer 4 - HA Edge to DC - L4 from DC</title><link>https://trustgrid.github.io/docs/tutorials/examples/layer3-layer4-ha-edge-to-dc-l4-from-dc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://trustgrid.github.io/docs/tutorials/examples/layer3-layer4-ha-edge-to-dc-l4-from-dc/</guid><description>
&lt;p>In this scenario, a clustered pair of active/passive edge nodes at each edge location connects to a pair of active/passive clustered gateway nodes at the data center. Layer 3 is utilized to allow an application server at the data center to access data at the edge location and a layer 4 service and connector is used at the data center to provide connectivitiy to an FTP service at edge location 1.&lt;/p>
&lt;p>This configuration allows for the loss of one node at each site without a loss of connectivity between the edge location and the data center. Upon node failover, the active cluster member is switched to the second node and the shared IP address follows, allowing traffic to continue to flow using the second node. Failback to the original active node happens when the node recovers from the event that caused the failover and the shared IP is moved back to the original node at that time.&lt;/p>
&lt;p>A route is added in the core router at the data center and edge locations for the Trustgrid subnet with the next hop set to the shared IP of the clustered nodes at each location. Inside NATs are used to translate local IP addresses to IP addresses on the Trustgrid network for layer 3 traffic. The L4 service on the data center node is configured so that L4 traffic to the FTP server will show up with the source IP as the shared IP for the cluster. The listening port of the L4 connector is accessible via the shared IP on the data center nodes, allowing for high-availability of the L4 connector.&lt;/p>
&lt;p>&lt;img src="l4-dc-topology.png" alt="Network Topology">&lt;/p>
&lt;h3 id="layer-4-configuration">Layer 4 Configuration&lt;/h3>
&lt;h4 id="service">Service&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Name&lt;/th>
&lt;th>Destination&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FTP&lt;/td>
&lt;td>Edge1FTP&lt;/td>
&lt;td>172.16.10.15:21&lt;/td>
&lt;td>FTP access on edge location 1 server&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="connector">Connector&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Listen Port&lt;/th>
&lt;th>Source Block&lt;/th>
&lt;th>Destination Node&lt;/th>
&lt;th>Remote Address&lt;/th>
&lt;th>IP&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FTP&lt;/td>
&lt;td>8021&lt;/td>
&lt;td>192.168.10.15/32&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;td>svc://Edge1FTP&lt;/td>
&lt;td>192.168.10.215&lt;/td>
&lt;td>L4 connector for FTP access to edge location 1 server&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="network-information">Network Information&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local Network CIDR&lt;/th>
&lt;th>Location&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.0/24&lt;/td>
&lt;td>Data center&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.0/24&lt;/td>
&lt;td>Edge location 1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.0/24&lt;/td>
&lt;td>Edge location 2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.0/24&lt;/td>
&lt;td>Edge location 3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="route-configuration">Route Configuration&lt;/h3>
&lt;p>In this example, the Trustgrid network is &lt;code>10.100.0.0/16&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Trustgrid Network CIDR&lt;/th>
&lt;th>Destination Node&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10.100.100.0/24&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.10.0/24&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.20.0/24&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10.100.30.0/24&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inside-nat-configuration">Inside NAT Configuration&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Local CIDR&lt;/th>
&lt;th>Network CIDR&lt;/th>
&lt;th>Node&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>192.168.10.15/32&lt;/td>
&lt;td>10.100.100.15/32&lt;/td>
&lt;td>DCCluster&lt;/td>
&lt;td>Application server behind DCCluster nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.10.15/32&lt;/td>
&lt;td>10.100.10.15/32&lt;/td>
&lt;td>EdgeCluster1&lt;/td>
&lt;td>MySQL server behind EdgeCluster1 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.15/32&lt;/td>
&lt;td>10.100.20.15/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>MySQL server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.20.20/32&lt;/td>
&lt;td>10.100.20.20/32&lt;/td>
&lt;td>EdgeCluster2&lt;/td>
&lt;td>SFTP server behind EdgeCluster2 nodes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>172.16.30.15/32&lt;/td>
&lt;td>10.100.30.15/32&lt;/td>
&lt;td>EdgeCluster3&lt;/td>
&lt;td>MySQL server behind EdgeCluster3 nodes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item></channel></rss>